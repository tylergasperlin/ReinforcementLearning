{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surrounded-wings",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-alberta",
   "metadata": {},
   "source": [
    "\n",
    "# Manually Creating a DQN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-porcelain",
   "metadata": {},
   "source": [
    "## Deep-Q-Learning\n",
    "In this notebook we will create our first Deep Reeinforcement Learning model, called Deep-Q-Network (DQN)\n",
    "We are again using a simple environment from openai gym. <br />\n",
    "However, you will soon see the enormous gain we will get by switching from standard Q-Learning to Deep Q Learning.\n",
    "\n",
    "In this notebook we again take a look at the CartPole problem (https://gym.openai.com/envs/CartPole-v1/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-destruction",
   "metadata": {},
   "source": [
    "Let us start by importing the necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239dd36",
   "metadata": {},
   "source": [
    "# Part 0: Imports\n",
    "\n",
    "Notice how we're importing the TF libraries here at the top together, in some rare instances, if you import them later on, you get strange bugs, so best just to import everything from Tensorflow here at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hybrid-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras.models import Sequential  # To compose multiple Layers\n",
    "from tensorflow.keras.layers import Dense  # Fully-Connected layer\n",
    "from tensorflow.keras.layers import Activation  # Activation functions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df24c33",
   "metadata": {},
   "source": [
    "# Part 1: The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fifty-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-exploration",
   "metadata": {},
   "source": [
    "Remember, the goal of the CartPole challenge was to balance the stick upright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb1e0b-73fe-47eb-aafa-9e0cdce27a31",
   "metadata": {},
   "source": [
    "env.reset()  # reset the environment to the initial state\n",
    "for _ in range(200):  # play for max 200 iterations\n",
    "    env.render()  # render the current game state on your screen\n",
    "    random_action = env.action_space.sample()  # chose a random action\n",
    "    env.step(random_action)  # execute that action\n",
    "env.close()  # close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf18acd",
   "metadata": {},
   "source": [
    "# Part 2: The Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-unknown",
   "metadata": {},
   "source": [
    "### Let us build our first Neural Network\n",
    "To build our network, we first need to find out how many actions and observation our environment has.\n",
    "We can either get those information from the source code (https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) or via the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "widespread-oxide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 possible actions and 4 observations\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]  # You can use this command to get the number of observations\n",
    "print(f\"There are {num_actions} possible actions and {num_observations} observations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-alaska",
   "metadata": {},
   "source": [
    "So our network needs to have an input dimension of 4 and an output dimension of 2.\n",
    "In between we are free to chose.\n",
    "\n",
    "Let's just say we want to use a four layer architecture:\n",
    "\n",
    "\n",
    "1. The first layer has 16 neurons\n",
    "2. The second layer has 32 neurons\n",
    "4. The fourth layer (output layer) has 2 neurons\n",
    "\n",
    "This yields 690 parameters\n",
    "$$ \\text{4 observations} * 16 (\\text{neurons}) + 16 (\\text{bias}) + (16*32) + 32 + (32*2)+2 = 690$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "insured-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 1, 16)             80        \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1, 16)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1, 32)             544       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 1, 32)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1, 2)              66        \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 1, 2)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, input_shape=(1, num_observations)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-sitting",
   "metadata": {},
   "source": [
    "Now we have our model which takes an observation as input and outputs a value for each action.\n",
    "The higher the value, the more likely that this value is a suitable action for the current observation\n",
    "\n",
    "As stated in the lecture, Deep-Q-Learning works better when using a target network.\n",
    "So let's just copy the above network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "advanced-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"34.ckt\")\n",
    "target_model = clone_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-terrorism",
   "metadata": {},
   "source": [
    "Now it is time to define our hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7991ba",
   "metadata": {},
   "source": [
    "# Part 3: Hyperparameters and Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "interim-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "epsilon = 1.0\n",
    "EPSILON_REDUCE = 0.995  # is multiplied with epsilon each epoch to reduce it\n",
    "LEARNING_RATE = 0.001 #NOT THE SAME AS ALPHA FROM Q-LEARNING FROM BEFORE!!\n",
    "GAMMA = 0.95\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-simple",
   "metadata": {},
   "source": [
    "Let us use the epsilon greedy action selection function once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "coordinate-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(model, epsilon, observation):\n",
    "    if np.random.random() > epsilon:\n",
    "        print(observation)\n",
    "        prediction = model.predict(observation)  # perform the prediction on the observation\n",
    "        action = np.argmax(prediction)  # Chose the action with the higher value\n",
    "    else:\n",
    "        action = np.random.randint(0, env.action_space.n)  # Else use random action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-local",
   "metadata": {},
   "source": [
    "As shown in the lecture, we need a replay buffer.\n",
    "We can use the **deque** data structure for this, which already implements the circular behavior.\n",
    "\n",
    "The *maxlen* argument specifies the number of elements the buffer can store between he overwrites them at the beginning\n",
    "\n",
    "The following cell shows an example usage of the deque function. You can see, that in the first example all values fit into the deque, so nothing is overwritten. \n",
    "\n",
    "In the second example, the deque is printed in each iteration. It can hold all values in the first five iterations but then needs to delete the oldest value in the deque to make room for the new value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "brilliant-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "---------------------\n",
      "deque([0], maxlen=5)\n",
      "deque([0, 1], maxlen=5)\n",
      "deque([0, 1, 2], maxlen=5)\n",
      "deque([0, 1, 2, 3], maxlen=5)\n",
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "deque([1, 2, 3, 4, 5], maxlen=5)\n",
      "deque([2, 3, 4, 5, 6], maxlen=5)\n",
      "deque([3, 4, 5, 6, 7], maxlen=5)\n",
      "deque([4, 5, 6, 7, 8], maxlen=5)\n",
      "deque([5, 6, 7, 8, 9], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "### deque examples\n",
    "deque_1 = deque(maxlen=5)\n",
    "for i in range(5):  # all values fit into the deque, no overwriting\n",
    "    deque_1.append(i)\n",
    "print(deque_1)\n",
    "print(\"---------------------\")\n",
    "deque_2 = deque(maxlen=5)\n",
    "\n",
    "# after the first 5 values are stored, it needs to overwrite the oldest value to store the new one\n",
    "for i in range(10):  \n",
    "    deque_2.append(i)\n",
    "    print(deque_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-august",
   "metadata": {},
   "source": [
    "Let's say we allow our replay buffer a maximum size of 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "thrown-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=20000)\n",
    "update_target_model = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-silicon",
   "metadata": {},
   "source": [
    "As mentioned in the lecture, action replaying is crucial for Deep Q-Learning. <br />\n",
    "The following cell implements one version of the action replay algorithm. <br />\n",
    "It uses the zip statement paired with the * (Unpacking Argument Lists) operator to create batches from the samples for efficient prediction and training.<br />\n",
    "The zip statement returns all corresponding pairs from each entry. <br />\n",
    "It might look confusing but the following example should clarify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "constant-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 7) (2, 5, 8) (3, 6, 9)\n"
     ]
    }
   ],
   "source": [
    "test_tuple = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
    "zipped_list = list(zip(*test_tuple))\n",
    "a, b, c = zipped_list\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-vegetation",
   "metadata": {},
   "source": [
    "Now it's time to write the replay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "scheduled-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_buffer, batch_size, model, target_model):\n",
    "    \n",
    "    # As long as the buffer has not enough elements we do nothing\n",
    "    if len(replay_buffer) < batch_size: \n",
    "        return\n",
    "    \n",
    "    # Take a random sample from the buffer with size batch_size\n",
    "    samples = random.sample(replay_buffer, batch_size)  \n",
    "    \n",
    "    # to store the targets predicted by the target network for training\n",
    "    target_batch = []  \n",
    "    \n",
    "    # Efficient way to handle the sample by using the zip functionality\n",
    "    zipped_samples = list(zip(*samples))  \n",
    "    states, actions, rewards, new_states, dones = zipped_samples  \n",
    "    \n",
    "    # Predict targets for all states from the sample\n",
    "    targets = target_model.predict(np.array(states))\n",
    "    \n",
    "    # Predict Q-Values for all new states from the sample\n",
    "    q_values = model.predict(np.array(new_states))  \n",
    "    \n",
    "    # Now we loop over all predicted values to compute the actual targets\n",
    "    for i in range(batch_size):  \n",
    "        \n",
    "        # Take the maximum Q-Value for each sample\n",
    "        q_value = max(q_values[i][0])  \n",
    "        \n",
    "        # Store the ith target in order to update it according to the formula\n",
    "        target = targets[i].copy()  \n",
    "        if dones[i]:\n",
    "            target[0][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            target[0][actions[i]] = rewards[i] + q_value * GAMMA\n",
    "        target_batch.append(target)\n",
    "\n",
    "    # Fit the model based on the states and the updated targets for 1 epoch\n",
    "    model.fit(np.array(states), np.array(target_batch), epochs=1, verbose=0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-testing",
   "metadata": {},
   "source": [
    "We need to update our target network every once in a while. <br />\n",
    "Keras provides the *set_weights()* and *get_weights()* methods which do the work for us, so we only need to check whether we hit an update epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "stainless-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_handler(epoch, update_target_model, model, target_model):\n",
    "    if epoch > 0 and epoch % update_target_model == 0:\n",
    "        target_model.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4755d07",
   "metadata": {},
   "source": [
    "# Part 4: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-michael",
   "metadata": {},
   "source": [
    "Now it is time to write the training loop! <br />\n",
    "First we compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sharing-position",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-mineral",
   "metadata": {},
   "source": [
    "Then we perform the training routine. <br />\n",
    "This might take some time, so make sure to grab your favorite beverage and watch your model learn. <br />\n",
    "Feel free to use our provided chekpoints as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "altered-interface",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Points reached: 21 - epsilon: 0.995 - Best: 21\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[[-0.09287532 -0.7877592   0.08816639  1.1571473 ]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_1/dense_3/Tensordot/GatherV2_1' defined at (most recent call last):\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/4p/q2w97g_n3xv8t1g03z0l35sw0000gn/T/ipykernel_8867/4034569020.py\", line 13, in <module>\n      action = epsilon_greedy_action_selection(model, epsilon, observation)\n    File \"/var/folders/4p/q2w97g_n3xv8t1g03z0l35sw0000gn/T/ipykernel_8867/3577742953.py\", line 4, in epsilon_greedy_action_selection\n      prediction = model.predict(observation)  # perform the prediction on the observation\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/layers/core/dense.py\", line 244, in call\n      outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])\nNode: 'sequential_1/dense_3/Tensordot/GatherV2_1'\nindices[0] = 2 is not in [0, 2)\n\t [[{{node sequential_1/dense_3/Tensordot/GatherV2_1}}]] [Op:__inference_predict_function_25822]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4p/q2w97g_n3xv8t1g03z0l35sw0000gn/T/ipykernel_8867/4034569020.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Select action acc. to strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy_action_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Perform action and get next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4p/q2w97g_n3xv8t1g03z0l35sw0000gn/T/ipykernel_8867/3577742953.py\u001b[0m in \u001b[0;36mepsilon_greedy_action_selection\u001b[0;34m(model, epsilon, observation)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# perform the prediction on the observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Chose the action with the higher value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_1/dense_3/Tensordot/GatherV2_1' defined at (most recent call last):\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/4p/q2w97g_n3xv8t1g03z0l35sw0000gn/T/ipykernel_8867/4034569020.py\", line 13, in <module>\n      action = epsilon_greedy_action_selection(model, epsilon, observation)\n    File \"/var/folders/4p/q2w97g_n3xv8t1g03z0l35sw0000gn/T/ipykernel_8867/3577742953.py\", line 4, in epsilon_greedy_action_selection\n      prediction = model.predict(observation)  # perform the prediction on the observation\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/tylergasperlin/opt/anaconda3/lib/python3.9/site-packages/keras/layers/core/dense.py\", line 244, in call\n      outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])\nNode: 'sequential_1/dense_3/Tensordot/GatherV2_1'\nindices[0] = 2 is not in [0, 2)\n\t [[{{node sequential_1/dense_3/Tensordot/GatherV2_1}}]] [Op:__inference_predict_function_25822]"
     ]
    }
   ],
   "source": [
    "best_so_far = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    observation = env.reset()[0]  # Get inital state\n",
    "    \n",
    "    # Keras expects the input to be of shape [1, X] thus we have to reshape\n",
    "    observation = observation.reshape([1, 4])  \n",
    "    done = False  \n",
    "    \n",
    "    points = 0\n",
    "    while not done:  # as long current run is active\n",
    "        \n",
    "        # Select action acc. to strategy\n",
    "        action = epsilon_greedy_action_selection(model, epsilon, observation)\n",
    "        \n",
    "        # Perform action and get next state\n",
    "        next_observation,reward,done,truncated,info = env.step(action)\n",
    "\n",
    "        next_observation = next_observation.reshape([1, 4])  # Reshape!!\n",
    "        replay_buffer.append((observation, action, reward, next_observation, done))  # Update the replay buffer\n",
    "        observation = next_observation  # update the observation\n",
    "        points+=1\n",
    "\n",
    "        # Most important step! Training the model by replaying\n",
    "        replay(replay_buffer, 32, model, target_model)\n",
    "\n",
    "    \n",
    "    epsilon *= EPSILON_REDUCE  # Reduce epsilon\n",
    "    \n",
    "    # Check if we need to update the target model\n",
    "    update_model_handler(epoch, update_target_model, model, target_model)\n",
    "    \n",
    "    if points > best_so_far:\n",
    "        best_so_far = points\n",
    "    if epoch %25 == 0:\n",
    "        print(f\"{epoch}: Points reached: {points} - epsilon: {epsilon} - Best: {best_so_far}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f86992-e095-4e88-82ea-040eb0771c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e969bb",
   "metadata": {},
   "source": [
    "# Part 5: Using Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()[0]\n",
    "for counter in range(300):\n",
    "    env.render()\n",
    "    \n",
    "    # TODO: Get discretized observation\n",
    "    action = np.argmax(model.predict(observation.reshape([1,4])))\n",
    "    \n",
    "    # TODO: Perform the action \n",
    "    observation,reward,done,truncated,info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(f\"done\")\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86929c-d52c-4464-ba40-0448d2c653c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
